{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# json 파일 생성 / train tesst split"
      ],
      "metadata": {
        "id": "1yFEJNooh0cj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWlZywCLM2-6",
        "outputId": "9752fcdc-82f0-463f-b156-4f02a9e18680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "import json"
      ],
      "metadata": {
        "id": "beH6N9y7PwSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 읽기\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Boaz/미니플젝1/data/news_edited.csv\", sep=\",\", engine=\"python\")\n",
        "\n",
        "# DataFrame의 모든 열 이름 출력\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oRYd_w9dr6I",
        "outputId": "ff794fce-7830-4a0e-ae44-ca129e6b45db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'title', 'text', 'label'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train_test_split 함수를 사용해 데이터 분할\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)  # test_size는 20%, random_state는 재현 가능성을 위해 설정d\n",
        "\n",
        "train_df.to_csv(\"train_data.csv\", index=False)\n",
        "test_df.to_csv(\"test_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "R-gi3ujThgOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_file = \"news2.model\"\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIybJG--PbzI",
        "outputId": "f762662c-cc43-4228-c07c-cae91e176b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" train data 준비 \"\"\"\n",
        "def prepare_train(vocab, infile, outfile):\n",
        "    df = pd.read_csv(infile, sep=\",\", engine=\"python\")\n",
        "    with open(outfile, \"w\") as f:\n",
        "        for index, row in df.iterrows():\n",
        "            text = row[\"text\"]\n",
        "            if type(text) != str:\n",
        "                continue\n",
        "            instance = { \"id\": row[\"Unnamed: 0\"], \"doc\": vocab.encode_as_pieces(text), \"label\": row[\"label\"] }\n",
        "            f.write(json.dumps(instance))\n",
        "            f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "q7tKHu_9P0-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_train(vocab, \"train_data.csv\", \"news_train.json\")\n",
        "prepare_train(vocab, \"test_data.csv\", \"news_test.json\")"
      ],
      "metadata": {
        "id": "DtVv7uEbP9_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model에 없는 단어 제외"
      ],
      "metadata": {
        "id": "AY7Gx64Ph9R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLGuqI6gjGQJ",
        "outputId": "ad324a54-2393-4cb3-eb13-156c1d5acf1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# 모델의 SentencePiece 파일 경로\n",
        "model_path = 'news2.model'\n",
        "\n",
        "# SentencePiece 모델 로드\n",
        "sp = spm.SentencePieceProcessor(model_file=model_path)\n",
        "\n",
        "# 어휘 목록 추출 (index와 함께)\n",
        "vocab_words = [sp.id_to_piece(i) for i in range(sp.get_piece_size())]\n",
        "\n",
        "# 어휘 목록을 집합으로 변환\n",
        "vocab_words_set = set(vocab_words)"
      ],
      "metadata": {
        "id": "l_cihR2eiCvc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# JSON 파일 경로\n",
        "json_path = 'news_test.json'\n",
        "\n",
        "# JSON 파일 읽기\n",
        "with open(json_path, 'r') as file:\n",
        "    data = [json.loads(line) for line in file]\n",
        "\n",
        "# 모델 어휘에 포함된 단어만 필터링\n",
        "filtered_data = []\n",
        "for entry in data:\n",
        "    if isinstance(entry[\"doc\"], list):\n",
        "        # 리스트를 문자열로 변환 후 단어 추출\n",
        "        text = ' '.join(entry[\"doc\"])\n",
        "    else:\n",
        "        text = entry[\"doc\"]\n",
        "\n",
        "    # 텍스트에서 단어 추출 (단어는 공백으로 분리된다고 가정)\n",
        "    words = text.split()\n",
        "\n",
        "    # 모델 어휘에 포함된 단어만 필터링\n",
        "    filtered_words = [word for word in words if word in vocab_words]\n",
        "\n",
        "    if filtered_words:\n",
        "        filtered_data.append({\n",
        "            \"id\": entry[\"id\"],\n",
        "            \"doc\": filtered_words,  # 필터링된 단어로 업데이트\n",
        "            \"label\": entry[\"label\"]\n",
        "        })\n"
      ],
      "metadata": {
        "id": "zAenh7ZmiHPJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 필터링된 데이터를 새로운 JSON 파일로 저장\n",
        "filtered_json_path = 'news2_test.json'\n",
        "\n",
        "with open(filtered_json_path, 'w') as f:\n",
        "    for entry in filtered_data:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "nt9fKUeUiI93"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# JSON 파일 경로\n",
        "json_path = 'news_train.json'\n",
        "\n",
        "# JSON 파일 읽기\n",
        "with open(json_path, 'r') as file:\n",
        "    data = [json.loads(line) for line in file]\n",
        "\n",
        "# 모델 어휘에 포함된 단어만 필터링\n",
        "filtered_data = []\n",
        "for entry in data:\n",
        "    if isinstance(entry[\"doc\"], list):\n",
        "        # 리스트를 문자열로 변환 후 단어 추출\n",
        "        text = ' '.join(entry[\"doc\"])\n",
        "    else:\n",
        "        text = entry[\"doc\"]\n",
        "\n",
        "    # 텍스트에서 단어 추출 (단어는 공백으로 분리된다고 가정)\n",
        "    words = text.split()\n",
        "\n",
        "    # 모델 어휘에 포함된 단어만 필터링\n",
        "    filtered_words = [word for word in words if word in vocab_words]\n",
        "\n",
        "    if filtered_words:\n",
        "        filtered_data.append({\n",
        "            \"id\": entry[\"id\"],\n",
        "            \"doc\": filtered_words,  # 필터링된 단어로 업데이트\n",
        "            \"label\": entry[\"label\"]\n",
        "        })\n"
      ],
      "metadata": {
        "id": "QiJZccUsjvyM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 필터링된 데이터를 새로운 JSON 파일로 저장\n",
        "filtered_json_path = 'news2_train.json'\n",
        "\n",
        "with open(filtered_json_path, 'w') as f:\n",
        "    for entry in filtered_data:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "WAjamLxbjvmQ"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}