{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '겨울은', '▁', '추워요', '.']\n",
      "['▁', '감기', '▁', '조심하세요', '.']\n",
      "torch.Size([2, 5])\n",
      "tensor([[7921,    1, 7921,    1, 7942],\n",
      "        [7921,    1, 7921,    1, 7942]])\n"
     ]
    }
   ],
   "source": [
    "vocab_file = \"news2.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)\n",
    "\n",
    "# 입력 texts\n",
    "lines = [\n",
    "  \"겨울은 추워요.\",\n",
    "  \"감기 조심하세요.\"\n",
    "]\n",
    "\n",
    "# text를 tensor로 변환(텍스트 데이터를 숫자로 allow computer to understand text)\n",
    "inputs = []\n",
    "for line in lines:\n",
    "  pieces = vocab.encode_as_pieces(line)\n",
    "  ids = vocab.encode_as_ids(line)\n",
    "  inputs.append(torch.tensor(ids))\n",
    "  print(pieces)\n",
    "\n",
    "# 입력 길이가 다르므로 입력 최대 길이에 맟춰 padding(0)을 추가 해 줌\n",
    "inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "# shape\n",
    "print(inputs.size())\n",
    "# 값\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding\n",
    "transformer uses input embedding and position embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input embedding\n",
    "입력 텍스트(단어, 문장 등)를 모델이 이해할 수 있는 숫자 벡터로 변환하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(vocab) # vocab count\n",
    "d_hidn = 128 # hidden size\n",
    "nn_emb = nn.Embedding(n_vocab, d_hidn) # embedding 객체\n",
    "\n",
    "input_embs = nn_emb(inputs) # input embedding\n",
    "print(input_embs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## position embedding\n",
    "\n",
    "- 각 position별도 angle 값을 구합니다.\n",
    "- 구해진 angle 중 짝수 index의 값에 대한 sin 값을 구합니다.\n",
    "- 구해진 angle 중 홀수 index의 값에 대한 cos 값을 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" sinusoid position embedding \"\"\"\n",
    "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
    "    def cal_angle(position, i_hidn):\n",
    "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "\n",
    "    return sinusoid_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
